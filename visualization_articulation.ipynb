{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "from models.ngp_wrapper import NGP_Prop_Art_Seg_Wrapper\n",
    "from dataset.pose_utils import quaternion_to_axis_angle, get_quaternion_axis_angle\n",
    "from config import get_opts\n",
    "from dataset.sapien import SapienParisDataset\n",
    "import math\n",
    "from pathlib import Path as P\n",
    "from dataset.io_utils import load_gt_from_json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "def draw_axis(pil_img, axis_info, c2w, K, thickness=8):\n",
    "    '''\n",
    "    info_type\n",
    "        options: gt or pred\n",
    "    '''\n",
    "    cv_img = np.array(pil_img)\n",
    "    # cv_img = cv2.cvtColor(cv_img, cv2.COLOR_RGB2BGR)\n",
    "    pred_info = axis_info['pred']\n",
    "    gt_info = axis_info['gt']\n",
    "    pred_pix = proj_axis(pred_info, c2w, K).round().detach().cpu().numpy().astype(np.int16)\n",
    "    cv2.arrowedLine(cv_img, pred_pix[0], pred_pix[1], thickness=thickness, color=(255, 0, 0))\n",
    "    \n",
    "    gt_pix = proj_axis(gt_info, c2w, K).round().detach().cpu().numpy().astype(np.int16)\n",
    "    cv2.arrowedLine(cv_img, gt_pix[0], gt_pix[1], thickness=thickness, color=(0, 255, 0))\n",
    "    \n",
    "    return Image.fromarray(cv_img)\n",
    "\n",
    "@torch.inference_mode()\n",
    "def proj_axis(axis_info, c2w, K):\n",
    "    '''\n",
    "    return [2, 2] in pixel coordinate\n",
    "    '''\n",
    "    # Add a dimension of ones to the point cloud to make it homogeneous\n",
    "    ones = torch.ones((axis_info.shape[0], 1), device=axis_info.device)\n",
    "    homogeneous_point_cloud = torch.cat((axis_info, ones), dim=1)\n",
    "    \n",
    "    # Transform the point cloud from world coordinates to camera coordinates\n",
    "    points_in_camera_coordinates = torch.inverse(c2w.to(axis_info)) @ homogeneous_point_cloud.t()\n",
    "    # Normalize the coordinates\n",
    "    points_in_camera_coordinates /= points_in_camera_coordinates[3, :].clone()\n",
    "    \n",
    "    # Project the points onto the 2D plane using the intrinsic matrix\n",
    "    projected_points = K.to(axis_info) @ points_in_camera_coordinates[:3, :]\n",
    "    \n",
    "    # Normalize the projected points\n",
    "    projected_points /= projected_points[2, :].clone()\n",
    "    \n",
    "    return projected_points[:2, :].T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_args\n",
    "fridge_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/fridge_end_to_start_f64/1714724128/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/fridge_end_to_start_f64/1714724128/eval/config.json\",\n",
    "    \"test_path\": \"vis/fridge\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n",
    "\n",
    "stapler_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/stapler_end_to_start_f100/1714808215/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/stapler_end_to_start_f100/1714808215/eval/config.json\",\n",
    "    \"test_path\": \"vis/stapler\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n",
    "\n",
    "oven_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/oven_start_to_end_f100/1714874463/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/oven_start_to_end_f100/1714874463/eval/config.json\",\n",
    "    \"test_path\": \"vis/oven\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n",
    "blade_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/blade_start_to_end_f100/1714857261/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/blade_start_to_end_f100/1714857261/eval/config.json\",\n",
    "    \"test_path\": \"vis/blade\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n",
    "storage_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/storage_end_to_start_f100/1714855450/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/storage_end_to_start_f100/1714855450/eval/config.json\",\n",
    "    \"test_path\": \"vis/storage\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n",
    "\n",
    "laptop_dict = {\n",
    "    \"ckpt_fname\": \"results_ablation/laptop_start_to_end_f100/1714857261/ckpt/best_ckpt.pth\",\n",
    "    \"cfg_file\": \"results_ablation/laptop_start_to_end_f100/1714857261/eval/config.json\",\n",
    "    \"test_path\": \"vis/laptop\",\n",
    "    \"fix_view\": 33,\n",
    "    \"num_step\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_vis(ckpt_fname, cfg_file, test_path, num_step=30, fix_view=None):\n",
    "\n",
    "    test_path = P(test_path)\n",
    "\n",
    "\n",
    "    test_path.mkdir(exist_ok=True, parents=True)\n",
    "    opts = get_opts(['--config', cfg_file])\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        \n",
    "    setattr(opts, 'device', device)\n",
    "    opts.pre_trained_weights = None\n",
    "    # num_step = 30\n",
    "\n",
    "    model = NGP_Prop_Art_Seg_Wrapper(config=opts, training=False, \n",
    "                                    ignore_empty=False, use_timestamp=False, use_se3=opts.use_se3)\n",
    "\n",
    "\n",
    "\n",
    "    gt_info = load_gt_from_json(opts.motion_gt_json, opts.state, opts.motion_type)\n",
    "\n",
    "    test_dataset = SapienParisDataset(\n",
    "            root_dir = opts.root_dir,\n",
    "            near = opts.near_plane,\n",
    "            far = opts.far_plane,\n",
    "            img_wh = opts.img_wh, \n",
    "            batch_size=opts.batch_size,\n",
    "            split='test',\n",
    "            render_bkgd='white',\n",
    "            state=opts.state\n",
    "        )\n",
    "\n",
    "    model.load_ckpt(ckpt_fname)\n",
    "    motion_list = []\n",
    "    axis_list = []\n",
    "    \n",
    "    vis_scale = 1\n",
    "    if 'fridge' in ckpt_fname:\n",
    "        vis_scale = 0.5\n",
    "    if opts.motion_type == 'r':\n",
    "        # gen_art_pose\n",
    "        for pose_param in model.pose_module_list:\n",
    "            pose_param.norm_Q()\n",
    "            cur_quat = pose_param.Q\n",
    "            axis_d, angle, deg = quaternion_to_axis_angle(cur_quat)\n",
    "            angle_detach = angle.cpu().detach()\n",
    "            # print(torch.arange(0, num_step+1) / (num_step))\n",
    "            if angle > 0:\n",
    "                cur_list = torch.arange(0, num_step+1) / (num_step) * angle_detach\n",
    "            else:\n",
    "                cur_list = angle_detach - torch.arange(0, num_step+1) / (num_step) * angle_detach\n",
    "            half_q_list = [get_quaternion_axis_angle(axis_d.cpu().detach().numpy(), cur_q) for cur_q in cur_list]\n",
    "            motion_list += [half_q_list]\n",
    "            axis_o = pose_param.axis_origin\n",
    "            sim = torch.nn.functional.cosine_similarity(axis_o.view(1, -1), axis_d.view(1, -1), dim=1)\n",
    "            \n",
    "            cur_axis = torch.stack([axis_o -sim*axis_d, axis_o + (1-vis_scale*sim)*axis_d])\n",
    "            axis_list += [cur_axis]\n",
    "    else:\n",
    "        for pose_param in model.pose_module_list:\n",
    "            # print(pose_param)\n",
    "            pose_param.norm_dir()\n",
    "            scale = pose_param.scale.cpu().detach()\n",
    "            axis_d = pose_param.dir\n",
    "            if scale > 0:\n",
    "                # cur_list = torch.arange(0, scale, step=scale/num_step)\n",
    "                cur_list = torch.arange(0, num_step+1) / (num_step ) * scale\n",
    "            else:\n",
    "                cur_list = scale - torch.arange(0, num_step+1) / (num_step ) * scale\n",
    "            # print(f'cur_list: {cur_list}')\n",
    "            motion_list += [cur_list]\n",
    "            # print(f'motion list: {motion_list}')\n",
    "            cur_axis = torch.stack([torch.zeros_like(axis_d), axis_d])\n",
    "            axis_list += [cur_axis]\n",
    "\n",
    "    # generate camera pose\n",
    "    theta_list = torch.arange(0, 2*math.pi, step=math.pi/20)\n",
    "    phi = torch.zeros_like(theta_list) * 0.25 * math.pi\n",
    "\n",
    "    K = test_dataset.K\n",
    "    pred_axis_info = axis_list[0]\n",
    "    print(vis_scale)\n",
    "    gt_axis_info = torch.stack([gt_info['axis_o'], gt_info['axis_o'] + vis_scale * gt_info['axis_d']])\n",
    "    axis_info = {\n",
    "        'pred': pred_axis_info,\n",
    "        'gt': gt_axis_info\n",
    "    }\n",
    "\n",
    "    model.load_ckpt(ckpt_fname)\n",
    "    # test_path = P(ckpt_fname).parent.parent / 'test'\n",
    "    print(axis_info)\n",
    "    static_path = test_path / 'target'\n",
    "    static_path.mkdir(exist_ok=True)\n",
    "    psnrs = []\n",
    "    gt_img_list = []\n",
    "    pred_img_list = []\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        test_data = test_dataset.__getitem__(i)\n",
    "        eval_dict = model.eval(test_data)\n",
    "        psnrs += [eval_dict['psnr'].item()]\n",
    "        img_gt = eval_dict['img_gt']\n",
    "        img_pred = eval_dict['img_pred']\n",
    "        \n",
    "        img_gt.save(static_path / f'img_gt_{i:04d}.png')\n",
    "        img_pred.save(static_path / f'img_pred_{i:04d}.png')\n",
    "        # print(axis_info)\n",
    "        img_arrow = draw_axis(img_pred, axis_info, test_data['c2w'].squeeze(0), K)\n",
    "        img_arrow.save(static_path / f'img_pred_arrow_{i:04d}.png')\n",
    "        gt_img_list += [img_gt]\n",
    "        pred_img_list += [img_arrow]\n",
    "    avg_psnr = sum(psnrs) / len(psnrs)\n",
    "    print(avg_psnr)\n",
    "\n",
    "    art_path = test_path / 'art'\n",
    "    art_path.mkdir(exist_ok=True)\n",
    "    total_art_part = len(model.pose_module_list)\n",
    "    art_img_list = []\n",
    "    dataset_len = len(test_dataset)\n",
    "    # print(pose_param.scale)\n",
    "    # print(motion_list[0])\n",
    "    # print(axis_info)\n",
    "    for i in tqdm(range(len(motion_list[0]))):\n",
    "        if fix_view is None:\n",
    "            test_data = test_dataset.__getitem__(i % dataset_len)\n",
    "        else:\n",
    "            test_data = test_dataset.__getitem__(fix_view)\n",
    "        \n",
    "        if opts.motion_type == 'r':\n",
    "            for p in range(total_art_part):\n",
    "                cur_Q = torch.Tensor(motion_list[p][i])\n",
    "                cur_pose_param = model.pose_module_list[p]\n",
    "                cur_Q = cur_Q.to(cur_pose_param.Q)\n",
    "                cur_pose_param.Q = torch.nn.Parameter(cur_Q)\n",
    "        else:\n",
    "            for p in range(total_art_part):\n",
    "                cur_scale = torch.Tensor(motion_list[p][i])\n",
    "                cur_pose_param = model.pose_module_list[p]\n",
    "                cur_scale = cur_scale.to(cur_pose_param.scale)\n",
    "                cur_pose_param.scale = torch.nn.Parameter(cur_scale)\n",
    "        \n",
    "        eval_dict = model.eval(test_data)\n",
    "        img_pred = eval_dict['img_pred']\n",
    "        # print(axis_info)\n",
    "        img_arrow = draw_axis(img_pred, axis_info, test_data['c2w'].squeeze(0), K, thickness=10)\n",
    "        img_arrow.save(art_path / f'img_pred_art_{i:04d}.png')\n",
    "        img_pred.save(art_path / f'img_pred_art_clean_{i:04d}.png')\n",
    "        art_img_list += [img_arrow]\n",
    "        pass\n",
    "\n",
    "    # gif_path = test_path / 'gif'\n",
    "    # gif_path.mkdir(exist_ok=True)\n",
    "    # gt_img_list[0].save(str(gif_path / 'gt.gif'), save_all=True, append_images=gt_img_list[1:], duration=5, optimize=False, loop=0)\n",
    "    # pred_img_list[0].save(str(gif_path / 'pred_target.gif'), save_all=True, append_images=pred_img_list[1:], duration=5, optimize=False, loop=0)\n",
    "    # art_img_list[0].save(str(gif_path / 'pred_art.gif'), save_all=True, append_images=art_img_list[1:], duration=5, optimize=False, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'pred': tensor([[-0.2553,  0.3327, -0.0066],\n",
      "        [-0.2592,  1.3327, -0.0077]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[-0.2471,  0.0000, -0.0077],\n",
      "        [-0.2471,  1.0000, -0.0077]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**laptop_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'pred': tensor([[-0.7505,  0.0731,  0.1030],\n",
      "        [-0.7568, -0.9268,  0.1013]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[-0.7521,  0.0000,  0.1050],\n",
      "        [-0.7521,  1.0000,  0.1050]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.25s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**stapler_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "{'pred': tensor([[ 0.1664,  0.2306, -0.1386],\n",
      "        [ 0.1669,  0.2342, -1.0403]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[0.1669, 0.2269, 0.4569],\n",
      "        [0.1669, 0.2269, 0.9569]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:01<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.83317230224609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.37s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**fridge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'pred': tensor([[ 0.4048,  0.0576, -0.4440],\n",
      "        [ 0.4025,  1.0576, -0.4497]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[ 0.4194, -0.6382, -0.4423],\n",
      "        [ 0.4194,  0.3618, -0.4423]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.33s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**oven_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'pred': tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9999,  0.0047, -0.0143]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[ 0.3823, -0.0146,  0.1356],\n",
      "        [ 1.3823, -0.0146,  0.1356]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**storage_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'pred': tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.9990,  0.0406, -0.0183]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>), 'gt': tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "save_vis(**blade_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_vis_multipart(ckpt_fname, cfg_file, test_path, num_step=30, fix_view=None):\n",
    "\n",
    "    test_path = P(test_path)\n",
    "\n",
    "\n",
    "    test_path.mkdir(exist_ok=True)\n",
    "    opts = get_opts(['--config', cfg_file])\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "        \n",
    "    setattr(opts, 'device', device)\n",
    "    opts.pre_trained_weights = None\n",
    "    # num_step = 30\n",
    "\n",
    "    model = NGP_Prop_Art_Seg_Wrapper(config=opts, training=False, \n",
    "                                    ignore_empty=False, use_timestamp=False, use_se3=opts.use_se3)\n",
    "\n",
    "\n",
    "\n",
    "    gt_info = load_gt_from_json(opts.motion_gt_json, opts.state, opts.motion_type)\n",
    "\n",
    "    test_dataset = SapienParisDataset(\n",
    "            root_dir = opts.root_dir,\n",
    "            near = opts.near_plane,\n",
    "            far = opts.far_plane,\n",
    "            img_wh = opts.img_wh, \n",
    "            batch_size=opts.batch_size,\n",
    "            split='test',\n",
    "            render_bkgd='white',\n",
    "            state=opts.state\n",
    "        )\n",
    "\n",
    "    model.load_ckpt(ckpt_fname)\n",
    "    motion_list = []\n",
    "    axis_list = []\n",
    "    if opts.motion_type == 'r':\n",
    "        # gen_art_pose\n",
    "        for pose_param in model.pose_module_list:\n",
    "            # motion_list = []\n",
    "            # axis_list = []\n",
    "            pose_param.norm_Q()\n",
    "            cur_quat = pose_param.Q\n",
    "            axis_d, angle, deg = quaternion_to_axis_angle(cur_quat)\n",
    "            print(f'angle = {angle}')\n",
    "            if angle > 0:\n",
    "                cur_list = torch.arange(0, angle.item(), step=angle.item()/num_step)\n",
    "            else:\n",
    "                cur_list = torch.arange(angle.item(), 0, step=angle.item()/num_step)\n",
    "            half_q_list = [get_quaternion_axis_angle(axis_d.cpu().detach().numpy(), cur_q) for cur_q in cur_list]\n",
    "            cur_q_list = half_q_list + half_q_list[::-1]\n",
    "            motion_list += [cur_q_list]\n",
    "            axis_o = pose_param.axis_origin\n",
    "            cur_axis = torch.stack([axis_o, axis_o + axis_d])\n",
    "            axis_list += [cur_axis]\n",
    "            # motion_lists += [motion_list]\n",
    "            # axis_lists += [axis_list]\n",
    "    else:\n",
    "        for pose_param in model.pose_module_list:\n",
    "            pose_param.norm_dir()\n",
    "            scale = pose_param.scale()\n",
    "            axis_d = pose_param.dir()\n",
    "            if scale > 0:\n",
    "                cur_list = torch.arange(0, scale, step=scale/num_step)\n",
    "            else:\n",
    "                cur_list = torch.arange(scale, 0, step=scale/num_step)\n",
    "            motion_list += [axis_d * cur_scale for cur_scale in cur_list]\n",
    "            cur_axis = torch.stack([torch.zeros_like(axis_d), axis_d])\n",
    "            axis_list += [cur_axis]\n",
    "\n",
    "    # generate camera pose\n",
    "    theta_list = torch.arange(0, 2*math.pi, step=math.pi/20)\n",
    "    phi = torch.zeros_like(theta_list) * 0.25 * math.pi\n",
    "\n",
    "    K = test_dataset.K\n",
    "    pred_axis_info = axis_list[0]\n",
    "    gt_axis_info = torch.stack([gt_info['axis_o'], gt_info['axis_o'] + gt_info['axis_d']])\n",
    "    axis_info = {\n",
    "        'pred': pred_axis_info,\n",
    "        'gt': gt_axis_info\n",
    "    }\n",
    "\n",
    "    model.load_ckpt(ckpt_fname)\n",
    "    # test_path = P(ckpt_fname).parent.parent / 'test'\n",
    "\n",
    "    static_path = test_path / 'target'\n",
    "    static_path.mkdir(exist_ok=True)\n",
    "    psnrs = []\n",
    "    gt_img_list = []\n",
    "    pred_img_list = []\n",
    "    for i in tqdm(range(len(test_dataset))):\n",
    "        test_data = test_dataset.__getitem__(i)\n",
    "        eval_dict = model.eval(test_data)\n",
    "        psnrs += [eval_dict['psnr'].item()]\n",
    "        img_gt = eval_dict['img_gt']\n",
    "        img_pred = eval_dict['img_pred']\n",
    "        \n",
    "        img_gt.save(static_path / f'img_gt_{i:04d}.png')\n",
    "        img_pred.save(static_path / f'img_pred_{i:04d}.png')\n",
    "        \n",
    "        img_arrow = draw_axis(img_pred, axis_info, test_data['c2w'].squeeze(0), K)\n",
    "        img_arrow.save(static_path / f'img_pred_arrow_{i:04d}.png')\n",
    "        gt_img_list += [img_gt]\n",
    "        pred_img_list += [img_arrow]\n",
    "    avg_psnr = sum(psnrs) / len(psnrs)\n",
    "    print(avg_psnr)\n",
    "\n",
    "    art_path = test_path / 'art'\n",
    "    art_path.mkdir(exist_ok=True)\n",
    "    total_art_part = len(model.pose_module_list)\n",
    "    art_img_list = []\n",
    "    dataset_len = len(test_dataset)\n",
    "    for i in tqdm(range(len(motion_list[0]))):\n",
    "        if fix_view is None:\n",
    "            test_data = test_dataset.__getitem__(i % dataset_len)\n",
    "        else:\n",
    "            test_data = test_dataset.__getitem__(fix_view)\n",
    "        \n",
    "        for p in range(total_art_part):\n",
    "            cur_Q = torch.Tensor(motion_list[p][i])\n",
    "            cur_pose_param = model.pose_module_list[p]\n",
    "            cur_Q = cur_Q.to(cur_pose_param.Q)\n",
    "            cur_pose_param.Q = torch.nn.Parameter(cur_Q)\n",
    "        \n",
    "        eval_dict = model.eval(test_data)\n",
    "        img_pred = eval_dict['img_pred']\n",
    "        img_arrow = draw_axis(img_pred, axis_info, test_data['c2w'].squeeze(0), K)\n",
    "        img_arrow.save(art_path / f'img_pred_art_{i:04d}.png')\n",
    "        art_img_list += [img_arrow]\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art_ngp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
